{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c973a42e-235f-4549-9934-82d0356fe06d",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Exercise 3 includes four problems that you need to solve with programming, and by providing answers to questions. For each problem you need to modify the notebook by adding your own solutions. Remember to save and commit your changes locally, and push your changes to GitHub after each major change! Regular commits will help you to keep track of your changes (and revert them if needed). Pushing your work to GitHub will ensure that you don't lose any work in case your computer crashes (can happen!).\n",
    "\n",
    "### Time allocation\n",
    "\n",
    "**Completing this exercise takes approximately: 6-10 hours** (based on previous year statistics). However, the time it takes can vary significantly from student to student, so we recommended that you start immediately working on the exercise to avoid issues meeting with the deadline.  \n",
    "\n",
    "### Due date\n",
    "\n",
    "You should submit (push) your Exercise answers to your personal Github repository approx. **2.5weeks after the practical session (submit by Sunday  26th Nov, 23:59)**. \n",
    "      \n",
    "### Start your exercise in CSC Notebooks\n",
    "\n",
    "Before you can start programming, you need to launch the CSC Notebook instance and clone your **personal copy of the Exercise repository** (i.e. something like `exercise-3-htenkanen`) there using Git. If you need help with this, [read the documentation on the course site](https://spatial-analytics.readthedocs.io/en/latest/lessons/L1/git-basics.html).\n",
    "\n",
    "### Working with Jupyter Notebooks\n",
    "\n",
    "Jupyter Notebooks are documents that can be used and run inside the JupyterLab programming environment (e.g. at [notebooks.csc.fi](https://notebooks.csc.fi/)) containing the computer code and rich text elements (such as text, figures, tables and links). \n",
    "\n",
    "**A couple of hints**:\n",
    "\n",
    "- You can **execute a cell** by clicking a given cell that you want to run and pressing <kbd>Shift</kbd> + <kbd>Enter</kbd> (or by clicking the \"Play\" button on top)\n",
    "- You can **change the cell-type** between `Markdown` (for writing text) and `Code` (for writing/executing code) from the dropdown menu above. \n",
    "\n",
    "See [**further details and help from here**](https://pythongis.org/part1/chapter-01/nb/04-using-jupyterlab.html). \n",
    "\n",
    "## Hints\n",
    "\n",
    "If there are general questions arising from this exercise, we will add hints to the course website under [Exercise 3 description](https://spatial-analytics.readthedocs.io/en/latest/lessons/L3/exercise-3.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e1091-08cb-4649-9155-562649c017f1",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "In this exercise, we use data from an area in Northern Sweden representing **copper content of rock samples**. \n",
    "\n",
    "- The data is provided to you as a GeoJSON file which is located in `data/copper_data.geojson`. \n",
    "- In the data, you see three attributes `F1`, `F2` and `F3`. First two are horizontal coordinates of sample point in Swedish rectangular coordinate system. The third field `F3` is the relative copper content in soil as mg/kg (ppm). \n",
    "- The data has been obtained from the [Geological Survey of Sweden (SGU)](https://www.sgu.se/en/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab942734-01aa-4a90-9955-264b7a3d5dd5",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "The following functions are provided for you to make things easier. **You need to execute/run this cell before you can use them!**\n",
    "Scroll down the notebook to find the Problems where you should use these functions for different tasks.\n",
    "Hint: You might want to study these functions carefully, and especially the documentation provided for them (as docstrings) to understand how to use them.\n",
    "\n",
    "There are altogether 6 functions below from which you will be using 5 in this exercise:\n",
    "\n",
    "1. `create_hexagon_grid()`\n",
    "2. `interpolate_idw()`\n",
    "3. `get_semivariogram_model()`\n",
    "4. `interpolate_ordinary_kriging()`\n",
    "5. `calculate_RMSE()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b4154-5693-4998-85c8-4ab598e566a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hexagon_grid(gdf, gridsize=60):\n",
    "    \"\"\"\n",
    "    Generates a Hexagon-grid GeoDataFrame having the same extent as the input gdf. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    gdf: gpd.GeoDataFrame with points\n",
    "        \n",
    "        A GeoDataFrame containing points (note: other geometry types won't work)\n",
    "        \n",
    "    gridsize: int \n",
    "    \n",
    "        Number of hexagons that should be created per side. Default 60.\n",
    "    \n",
    "    \"\"\"\n",
    "    import geopandas as gpd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from shapely.geometry import Polygon\n",
    "    import numpy as np\n",
    "\n",
    "    # Get coordinates\n",
    "    x = gdf.geometry.x.values\n",
    "    y = gdf.geometry.y.values\n",
    "\n",
    "    # Create the hexagons\n",
    "    plt.axis('off')\n",
    "    collection = plt.hexbin(x,y, gridsize=gridsize, \n",
    "                           alpha=0, linewidths=0)\n",
    "    \n",
    "    # Parse the hexagon geometries into Shapely Polygons\n",
    "    hex_polys = collection.get_paths()[0].vertices\n",
    "    hex_array = []\n",
    "    for xs, ys in collection.get_offsets():\n",
    "        hex_x = np.add(hex_polys[:,0],  xs)\n",
    "        hex_y = np.add(hex_polys[:,1],  ys)\n",
    "        hex_array.append(Polygon(np.vstack([hex_x, hex_y]).T))\n",
    "        \n",
    "    return gpd.GeoDataFrame({'geometry': hex_array}, crs=gdf.crs)\n",
    "\n",
    "\n",
    "def interpolate_idw(known_locations_gdf, \n",
    "                    unknown_locations_gdf, \n",
    "                    value_column, \n",
    "                    target_column=\"predicted_idw\",\n",
    "                    n_neighbors=-1,\n",
    "                    power=2\n",
    "                   ):\n",
    "    \"\"\"\n",
    "    A helper function to do IDW interpolation for locations defined in 'unknown_points_gdf'.\n",
    "    The values in column `value_column' of the 'known_points_gdf' are used as the basis for doing the interpolation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    known_locations_gdf : gpd.GeoDataFrame\n",
    "        \n",
    "        GeoDataFrame containing the known values at given points that are used as the basis for the interpolation.\n",
    "        The geometries can be either points or polygons (a centroid of Polygon will be extracted). \n",
    "        \n",
    "    unknown_locations_gdf : gpd.GeoDataFrame\n",
    "        \n",
    "        GeoDataFrame containing the unknown locations. The geometries can be either points or \n",
    "        polygons (a centroid of Polygon will be extracted). The values for these locations will be predicted using the \n",
    "        Inverse Distance Weighting interpolation method. \n",
    "    \n",
    "    value_column : str\n",
    "        \n",
    "        The value column in the `known_points_gdf' containing the known values (ùë§ùëñ) used for prediction.\n",
    "        \n",
    "    target_column : str\n",
    "        \n",
    "        The column where the predicted value (ùëßÃÇ) will be stored.\n",
    "        \n",
    "    n_neighbors : int\n",
    "        \n",
    "        Number of neighbors to use for weighting. By default, all neighbors are used in the prediction (i.e. value: -1)\n",
    "        \n",
    "    power : int\n",
    "    \n",
    "        The power (ùõΩ) that defines the distance decay function (by default: 2). Higher power value emphasize the influence of the \n",
    "        points nearest to the unknown point. A smaller power value gives more equal influence of also more distant points.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    from pyinterpolate.idw import inverse_distance_weighting\n",
    "    \n",
    "    # Parse the pointset for the known locations (training dataset)\n",
    "    known_points = get_pointset_for_interpolation(known_locations_gdf, value_column)\n",
    "\n",
    "    # Create empty column for interpolated values\n",
    "    unknown_locations_gdf[target_column] = None\n",
    "\n",
    "    # Interpolate the values with the test dataset\n",
    "    for row in unknown_locations_gdf.itertuples():\n",
    "        index = row.Index\n",
    "        point = np.array([row.geometry.centroid.x, row.geometry.centroid.y])\n",
    "        # Do the prediction using IDW\n",
    "        prediction = inverse_distance_weighting(known_points, \n",
    "                                                point, \n",
    "                                                number_of_neighbours=n_neighbors, \n",
    "                                                power=power)\n",
    "        # Assign the value to the result\n",
    "        unknown_locations_gdf.loc[index, target_column] = round(prediction, 4)\n",
    "        \n",
    "    return unknown_locations_gdf\n",
    "\n",
    "\n",
    "def get_semivariogram_model(known_locations_gdf, \n",
    "                            value_column, \n",
    "                            search_radius, \n",
    "                            max_range,\n",
    "                            weighting,\n",
    "                            n_ranges,\n",
    "                            nugget=0\n",
    "                           ):\n",
    "    \n",
    "    \"\"\"\n",
    "    A helper function to find optimal semivariogram model for given points in 'gdf'.\n",
    "    The values in column `value_column' of the 'gdf' are used as the empirical basis for finding the semivariogram model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    known_locations_gdf : gpd.GeoDataFrame\n",
    "        \n",
    "        GeoDataFrame containing the known values at given points that are used as the basis for the interpolation.\n",
    "        The geometries can be either points (preferred) or polygons (a centroid of Polygon will be extracted). \n",
    "        \n",
    "    value_column : str\n",
    "        \n",
    "        The value column in the `gdf' containing the known values.\n",
    "        \n",
    "    search_radius : int\n",
    "        \n",
    "        The column where the predicted value (ùëßÃÇ) will be stored.\n",
    "        \n",
    "    max_range : int\n",
    "        \n",
    "        Number of neighbors to use for weighting. By default, all neighbors are used in the prediction (i.e. value: -1)\n",
    "    \n",
    "    n_ranges : int\n",
    "    \n",
    "        The model optimization algorithm divides the study extent into n number of ranges \n",
    "        and tests the theoretical model output against the experimental variogram for each range.\n",
    "        \n",
    "    nugget : float\n",
    "        \n",
    "        A nugget allows for the variogram to assume some measurement error in the original observations.\n",
    "        Presence of a nugget means that any two observations sampled arbitrarily closely will not necessarily \n",
    "        have the same value. It can somewhat be thought as possible \"measurement error\" in the original observations. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    Theoretical semivariogram model\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    from pyinterpolate.variogram.empirical.experimental_variogram import calculate_semivariance\n",
    "    from pyinterpolate import TheoreticalVariogram, build_theoretical_variogram, build_experimental_variogram\n",
    "    \n",
    "    # Extract x, y and z values from the gdf\n",
    "    known_points = get_pointset_for_interpolation(known_locations_gdf, value_column)\n",
    "    \n",
    "    # Create experimental semivariogram (based on the known values at given locations)\n",
    "    exp_semivar = build_experimental_variogram(input_array=known_points, step_size=search_radius, max_range=max_range)\n",
    "\n",
    "    # Fit data into a theoretical model\n",
    "    semivar = TheoreticalVariogram()\n",
    "    fitted = semivar.autofit(experimental_variogram=exp_semivar, \n",
    "                         model_types='all', \n",
    "                         nugget=nugget,\n",
    "                         number_of_ranges=n_ranges,\n",
    "                         deviation_weighting=weighting,\n",
    "                        )\n",
    "    semivar.plot()\n",
    "    print(\"[INFO] RMSE of the fitted model:\", semivar.rmse)\n",
    "\n",
    "    return semivar\n",
    "    \n",
    "\n",
    "def interpolate_ordinary_kriging(known_locations_gdf, \n",
    "                                 unknown_locations_gdf, \n",
    "                                 model, \n",
    "                                 value_col,\n",
    "                                 target_col, \n",
    "                                 error_col, \n",
    "                                 neighbors_range=None,\n",
    "                                 n_neighbors=8):\n",
    "    \"\"\"\n",
    "    A helper function to do Ordinary Kriging interpolation for locations defined in 'gdf' \n",
    "    based on given semivariogram 'model'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    known_locations_gdf : gpd.GeoDataFrame\n",
    "        \n",
    "        GeoDataFrame containing the known values at given points that are used as the basis for the interpolation.\n",
    "        The geometries can be either points or polygons (a centroid of Polygon will be extracted). \n",
    "        \n",
    "    unknown_locations_gdf : gpd.GeoDataFrame\n",
    "        \n",
    "        GeoDataFrame containing the unknown locations. The geometries can be either points or \n",
    "        polygons (a centroid of Polygon will be extracted). The values for these locations will be predicted using the \n",
    "        Inverse Distance Weighting interpolation method. \n",
    "        \n",
    "    model : pyinterpolate.TheoreticalVariogram\n",
    "    \n",
    "        Theoretical Variogram used for data interpolation. Use `get_semivariogram_model()` function to generate this model.\n",
    "        \n",
    "    value_col : str\n",
    "    \n",
    "        The value attribute in `known_locations_gdf` containing information measurements.\n",
    "        \n",
    "    target_col : str\n",
    "        \n",
    "        The column where the predicted values will be stored in the `unknown_locations_gdf`.\n",
    "        \n",
    "    error_col : str\n",
    "        \n",
    "        The column where the standard errors will be stored in the `unknown_locations_gdf`.\n",
    "        \n",
    "    neighbors_range : float\n",
    "    \n",
    "        None or float, the maximum distance for searching neighbors for a point. \n",
    "        If None is given, then the range is selected from the theoretical_model rang attribute.\n",
    "        \n",
    "    n_neighbors : int\n",
    "        \n",
    "        Number of neighbors to use for weighting. By default, 8 neighbors are used in the prediction (i.e. value: 8)\n",
    "        \n",
    "    \"\"\"\n",
    "    from pyinterpolate import kriging\n",
    "    import numpy as np\n",
    "    \n",
    "    # Parse the points with known observations\n",
    "    known_points = get_pointset_for_interpolation(known_locations_gdf, value_col)\n",
    "    \n",
    "    # Parse the coordinates of the unknown locations\n",
    "    unknown_points = np.vstack([unknown_locations_gdf.geometry.centroid.x.values, \n",
    "                                unknown_locations_gdf.geometry.centroid.y.values]).T\n",
    "    \n",
    "    # The Kriging interpolation returns various things (as a tuple)\n",
    "    # Here, we unpack the values into their own variables.\n",
    "    # 'predicted' value for given location, standard 'error' for the estimate\n",
    "    # 'weights' that were applied for given point \n",
    "    results = kriging(observations=known_points,\n",
    "                      theoretical_model=model,\n",
    "                      points=unknown_points,\n",
    "                      how=\"ok\",\n",
    "                      neighbors_range=neighbors_range,\n",
    "                      no_neighbors=n_neighbors,\n",
    "                   ).T\n",
    "\n",
    "    predicted, error, estimated_mean, weights = results[0], results[1], results[2], results[3]\n",
    "\n",
    "    # Store the results\n",
    "    unknown_locations_gdf[target_col] = np.round(predicted, 2)\n",
    "    unknown_locations_gdf[error_col] = error\n",
    "    \n",
    "    return unknown_locations_gdf\n",
    "\n",
    "\n",
    "\n",
    "def calculate_RMSE(gdf, validation_col=\"F3\", prediction_col=\"predicted_value\"):\n",
    "    \"\"\"\n",
    "    A helper function to calculate the Root Mean Square Error (RMSE) for predicted values. \n",
    "    The predictions are compared against the known values (validation). The smaller the value, the better.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    gdf: gpd.GeoDataFrame\n",
    "        \n",
    "        GeoDataFrame that should contain both the observed and predicted values.\n",
    "        \n",
    "    validation_col : str\n",
    "        \n",
    "        Column name for the observed values (i.e. the values that we know beforehand)\n",
    "        \n",
    "    prediction_col : str\n",
    "        \n",
    "        Column name for the predicted values (i.e. the values that were estimated using interpolation)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return mean_squared_error(gdf[validation_col].values, gdf[prediction_col].values, squared=False)\n",
    "\n",
    "\n",
    "def get_pointset_for_interpolation(gdf, value_column):\n",
    "    \"\"\"\n",
    "    A helper function to extract a pointset (numpy arrays) for interpolation. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    gdf: gpd.GeoDataFrame \n",
    "        \n",
    "        Input GeoDataFrame from which the pointset for interpolation are parsed. The x and y coordinates\n",
    "        are parsed from the geometry, and the z values are parsed from the selected column.\n",
    "        In case the input geometries are polygons, a centroid of the given geometry is returned.\n",
    "        \n",
    "    value_column : str\n",
    "        \n",
    "        The name of the column that contains the values (z) for the attribute of interest \n",
    "        (e.g. the amount of copper observed at given location). \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    # Extract x, y coordinates and the attribute (z) values from the gdf \n",
    "    x = gdf.geometry.centroid.x.values\n",
    "    y = gdf.geometry.centroid.y.values\n",
    "    z = gdf[value_column].values\n",
    "    return np.array([x, y, z]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06161edf-a281-45b7-bdd9-27883df7a54d",
   "metadata": {},
   "source": [
    "## Problem 1 - Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d087e923-751e-4754-b9b6-7792496546fe",
   "metadata": {},
   "source": [
    "### Task 1.1 - Divide the data into training and test datasets (4 points)\n",
    "\n",
    "**1. Read the data** file `data/copper_data.geojson` with geopandas into a variable called `data`(*the result will be a GeoDataFrame*).\n",
    "\n",
    "**2. Generate an interactive map** using the [`.explore()` function](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html) of geopandas, in which you should visualize the copper content in the column `\"F3\"`. You can use the classification scheme `quantiles` to better differentiate the patterns in the data (pass `scheme=\"quantiles\"` parameter into the `.explore()` function). See an example map below. \n",
    "\n",
    "**3. Divide the data into training and test datasets** by randomly picking 75 % of the data for training and 25 % for test:\n",
    "  - Store the training data into variable `train` and test data into variable `test`\n",
    "  - **Hint 1**: For random sampling the 75 % of the data into variable `train`, you can use the `.sample()` method of pandas (see [docs for help](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html))\n",
    "  - **Hint 2**: For selecting the remaining 25 % of the data, you should pick the rows the `data` which were **not** selected into the 75 % random sample. For doing this, you can take advantage of the index values in the `data` and `train`, and conduct an inverse selection using tilde (`~`) and `isin()` method. See [this StackOverflow answer](https://stackoverflow.com/a/44318806) for help.\n",
    "\n",
    "**4. Genererate an interactive map** which shows the training points with red color and the test points with blue color. See an example map below.\n",
    "  - **Hint:** Adding multiple layers with `.explore()` is possible as shown in the [last example on this page](https://geopandas.org/en/stable/docs/user_guide/interactive_mapping.html#Interactive-mapping). The examples on the page also shows how you can adjust different visualization parameters, such as color of the points.\n",
    "\n",
    "\n",
    "![Copper content](img/copper_content.png)\n",
    "*Example 1: The copper content visualized using quantiles classification (step 2 above).*\n",
    "\n",
    "![Training and test points](img/training_and_test_points.png)\n",
    "\n",
    "*Example 2: Randomly sampled points for training (75 %) and testing (25 %). Notice that the pattern you will see is likely different because they are picked randomly. (step 4 above)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e58dd-7617-4dc6-a576-cb6817b2d889",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3da70ac79f5424566da33104f675f1ae",
     "grade": true,
     "grade_id": "cell-240332c7c9c4521f",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# REPLACE THE ERROR BELOW WITH YOUR OWN CODE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa520f4e-03f7-472f-b079-b0c95ac42563",
   "metadata": {},
   "source": [
    "### Task 1.2 - Generate a hexagon grid (2 points)\n",
    "\n",
    "The hexagon grid will be used for producing continuous interpolation surface in the problems 2 and 3.\n",
    "\n",
    "**1. Generate a hexagon grid** (as below) by using the `create_hexagon_grid()` function provided for you in the [Helper functions](#Helper-functions). **Store the result** into variable `grid`. You can use the default gridsize (i.e. 60). \n",
    "  - Hint: Read the function docstring in [Helper functions](#Helper-functions) to understand how it works.\n",
    "**2. Generate an interactive map** showing the grid using `.explore()` that should look something like below:\n",
    "\n",
    "![Hexagon grid](img/hexagon_grid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf09e0-e2c7-40ee-81e3-edf2df9e3973",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9898cb1c27aca16ea2360ee59832eef0",
     "grade": true,
     "grade_id": "cell-07625de61096969e",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# REPLACE THE ERROR BELOW WITH YOUR OWN CODE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647530c-efba-49b5-90a7-0ba5a598499f",
   "metadata": {},
   "source": [
    "## Problem 2 - Interpolation with IWD\n",
    "\n",
    "In this problem, the objective is to conduct interpolation using Inverse Distance Weighting algorithm, and test how changing the `power` influences the results and accuracy of the prediction. All the interpolations that we do in this exercise are all based on the [`pyinterpolate` -library](https://pyinterpolate.readthedocs.io/en/latest/).\n",
    "\n",
    "### Task 2.1 - Find out the best performing $\\beta$ in IDW (3 points)\n",
    "\n",
    "**1. Use the `interpolate_idw()` function to conduct IDW interpolation** which is provided for you in [Helper functions](#Helper-functions) (read the documentation/docstring of the function for further details):\n",
    "\n",
    "  - You should use the `train` dataset as the known data points\n",
    "  - Use the `test` dataset as the points which are unknown, i.e. the predictions will be done for those locations (and saved into this table)\n",
    "    - Notice that in reality, we do know the real copper values for the points in `test` but here we \"pretend\" that we don't know them, because in this way we can assess the accuracy of our prediction.\n",
    "  - The column `\"F3\"` in the `train` dataset contains the values (copper content) that we want to use in prediction\n",
    "  - Use **a power of 1** in the interpolation (i.e. $\\beta$)\n",
    "  - Specify that you want to store the result in a **column called `\"idw_power_1\"`**\n",
    "  - Store the result from this function into a variable `test` (i.e. the `test` dataset that was passed as one of the argument will be updated)\n",
    "    \n",
    "**2. Calculate the Root Mean Square Error (RMSE) for the prediction** using the `calculate_RMSE()` -function that is provided for you in [Helper functions](#Helper-functions) (read the documentation/docstring of the function for further details how to use it):\n",
    "\n",
    "  - Pass the result from step 1 (i.e. should be the variable `test`)\n",
    "  - Use the `\"F3\"` column for the validation\n",
    "  - Use the column `\"idw_power_1\"` as the `prediction_col`\n",
    "  - Store the result into a new variable called `idw_p1`\n",
    "  - Print out the RMSE for the interpolation showing how well the predicted values compare against the known values. The lower the RMSE, the better the prediction.\n",
    "  \n",
    "**3. Repeat the steps 1 and 2** and test how **changing the power** ($\\beta$) influences the accuracy of the result. \n",
    "\n",
    "  - Repeat the analysis with **power values: 2,3 and 4**. Store the results into variables `idw_p2`, `idw_p3` and `idw_p4` accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5387426-532e-4534-923a-39659554e21c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af88c818fedd14d9995c341dd4732ffe",
     "grade": true,
     "grade_id": "cell-c944114547702dbd",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# REPLACE THE ERROR BELOW WITH YOUR OWN CODE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a1d469-9f3f-467c-a59d-d0c10a45651e",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "*Correct answers gives you 0.5 points allocated to problem 2*.\n",
    "\n",
    " - Question 2.1: Which $\\beta$ value produced the **best** result in your analysis above? \n",
    " - Question 2.2: Which $\\beta$ value produced the **worst** result? Why do you think it performs poorly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29a991-44c5-40ef-9e9d-3c6f1a76f629",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "Answer to the questions by adding text after the `Answer` bullet points below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f2d104-53c9-43ac-bf3a-926062e4c63d",
   "metadata": {},
   "source": [
    "(hint: double-click this cell to activate editing)\n",
    "\n",
    "- Answer for Q2.1:\n",
    "- Answer for Q2.2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce540114-8abb-480f-b24f-f694dc7c9ba7",
   "metadata": {},
   "source": [
    "### Task 2.2 - Create a map with IDW predictions (2 points)\n",
    "\n",
    "Use the power that had the best performance in the previous step, and create a map using the hexagon grid created in Task 1.2.\n",
    "\n",
    "**1. Use the `interpolate_idw()` function** and:\n",
    "  \n",
    "  - use the `train` dataset as the known locations\n",
    "  - use the hexagon `grid` as the unknown locations to which we will predict values using IDW\n",
    "  - use the $\\beta$ that performed best\n",
    "  - use again the `\"F3\"` as the column for known values\n",
    "  - specify that the results should be stored into column `idw_power_X`, where you should replace the `X` with the power that you choose to use\n",
    "  - store the result from the function into variable `grid` (i.e. we will update the hexagon grid created earlier)\n",
    "  \n",
    "**2. Create a map with the hexagon grid** where the color of the cell should be the interpolated value from step 1. Use a colormap `\"Reds\"` that you can define with parameter `cmap`. If you want, you can add the `test` points on top of this hexagon grid which allows you to easily evaluate visually how well the predictions have worked at different locations. As a result, you should get something like the map below.\n",
    "\n",
    "  - **Hint:** Adding multiple layers with `.explore()` is possible as shown in the [last example on this page](https://geopandas.org/en/stable/docs/user_guide/interactive_mapping.html#Interactive-mapping). The examples on the page also shows how you can adjust different visualization parameters, such as color of the points.\n",
    "  \n",
    "![IDW interpolation](img/idw_interpolation.png)\n",
    "\n",
    "*Example map how the result from IDW interpolation should look like (approximately).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676685cf-ac82-4bcd-aef1-44820ad2be1f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14516d9b402b065cb74814cac7ddcd1f",
     "grade": true,
     "grade_id": "cell-b59a65e4867c3d90",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# REPLACE THE ERROR BELOW WITH YOUR OWN CODE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890e039-5ae3-42af-abf3-47b00b942d68",
   "metadata": {},
   "source": [
    "## Problem 3 - Interpolation with Kriging\n",
    "\n",
    "In this problem, we will continue working with the same data and use **Ordinary Kriging** to predict the values (copper content) for unknown locations.\n",
    "To do this, we will:\n",
    "\n",
    "1. Check whether the data is normally distributed. If not:\n",
    "2. Transform the data by taking a log of the values, which helps to \"normalize\" the data\n",
    "3. Find the optimal semivariogram model for the data, and test how changing the parameters influence the model fit\n",
    "4. Conduct Ordinary Kriging for the `test` dataset with the selected model and validate the result by calculating the RMSE \n",
    "5. Visualize the interpolation result as well as the standard errors that shows where the prediction works well and where not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382eb53-4200-4e09-98df-a5516a855ff7",
   "metadata": {},
   "source": [
    "### Task 3.1 - Testing the assumptions (2 points)\n",
    "\n",
    "**1. Is the data normally distributed?**:\n",
    "\n",
    "  - Plot a histogram of the `\"F3\"` values using the `.plot.hist()` function of pandas ([see docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.hist.html) for help). Use 30 bins which helps showing the differences at different value classes. \n",
    "  - To make a better visual assessment of the normality, also make a [Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot) that helps to estimate whether your data points follow normal distribution. For this, you can use the [`qqplot()` -function](https://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.ProbPlot.qqplot.html) from the `statsmodels` library. See [this StackOverflow answer](https://stackoverflow.com/a/50677734) for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75298c-c077-4b53-a81c-c9bfdca76b57",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c603fd1a78ae8306c63578ff64b2622",
     "grade": true,
     "grade_id": "cell-ff055018306b52d5",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# REPLACE THE ERROR BELOW WITH YOUR OWN CODE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff52877-da25-43a7-9e04-2ae7965c08b7",
   "metadata": {},
   "source": [
    "**2. Log transform the data** (if it does not follow normal distribution):\n",
    "\n",
    "In case your data was not normally distributed, it needs to be transformed in such a way that it follows normal distribution. There are a few different approaches for doing this, and one of the most common approaches is to log-transform your data which helps to make the data as \"normal\" as possible. (*However, note that log-transform is not always recommended e.g. [with count data having zeros](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2010.00021.x)*).\n",
    "\n",
    "  - You can log-transform you data by using the  [`np.log()` -function](https://numpy.org/doc/stable/reference/generated/numpy.log.html) and applying it for the `\"F3\"` column. \n",
    "  - Store the data into a new column called `\"log_F3\"`\n",
    "  - Test with Q-Q plot and histogram whether the data now follows better the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79deb591-d977-4d49-85bd-7cf4cf9c228e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d7aa70f2b11bc1cd647e648651c811b",
     "grade": true,
     "grade_id": "cell-1697942f217a68dd",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Add your code here IF NEEDED\n",
    "\n",
    "# REPLACE THE ERROR BELOW WITH YOUR OWN CODE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cdca88-7afb-47a8-b0e3-b7385af9032c",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "*Sufficient description gives you 0.5 points allocated to problem 3*.\n",
    "\n",
    "- Question 3.1: Based on the plots you made above, do you think the data follows normal distribution? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2bd092-f979-410d-93b4-57c319141861",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "Answer to the questions by adding text after the `Answer` bullet point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f792f-57e3-436c-8355-ef4c53d629db",
   "metadata": {},
   "source": [
    "(hint: double-click this cell to activate editing)\n",
    "\n",
    "- Answer for Q3.1: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f95a25-2424-4038-82b1-94e316bb303a",
   "metadata": {},
   "source": [
    "## Task 3.2 - Find optimal semivariogram model (2 points)\n",
    "\n",
    "As a background information for this task, you can check the [documentation](https://pyinterpolate.readthedocs.io/en/stable/usage/tutorials/Semivariogram%20Estimation%20%28Basic%29.html#4)-Set-automatically-semivariogram-model) to understand how the optimal model can be found using the `pyinterpolate` library. Also it is **highly recommended** that you check the [slides for geostatistics](https://spatial-analytics.readthedocs.io/en/latest/lessons/L2/geostatistics-kriging.html) and [watch the lesson video](https://spatial-analytics.readthedocs.io/en/latest/lessons/L2/overview.html#lesson-videos), in case you want to revise what was said about the things relating to the questions below.\n",
    "\n",
    "**1. Investigate your data** using the interactive maps done in earlier tasks, and update the variables (also just test how changing the values influences the result!):\n",
    "    \n",
    "  - Think what would be a good **search radius** for the data? The search radius defines the distance between lags within each points are included in the calculations. \n",
    "    - **Modify the value for the `search_radius` variable below** (should be in meters). \n",
    "    - **Question 3.2: Describe why/how did you end up selecting the given search radius value?** (answer below to the \"Answers\" cell)\n",
    "  \n",
    "  - Investigate the interactive maps, and think what would be the maximum possible distance between two points in the dataset? \n",
    "    - **Update the `max_range` variable** below based on your findings (should be in meters).\n",
    "    - Hint: take advantage of the scalebar on the bottom left corner of the interactive maps and look the distances between points. The map and scalebar updates when you zoom in/out. \n",
    "  \n",
    "  - Think whether the influence of the nearest points for given location should be **weighted in some way**? \n",
    "  \n",
    "    - **Update the `weighting` variable** below (possible values are: equal, closest, furthest) and test different values for it. \n",
    "    - **Question 3.3**: Which of them works the best? Describe why you ended up using specific `weighting` parameter in your analysis? (answer below to the \"Answers\" cell)\n",
    "    \n",
    "  - Test whether allowing nugget, i.e. some error in the original measurements influences the fit of the Theoretical Model:\n",
    "  \n",
    "    - **Update the `nugget` variable** below (test with values: 0, 0.02, 0.05) and test how it influences the fit of your model.\n",
    "    - **Question 3.4**: Which of the nugget values produces the \"best\" result? Why? (answer below to the \"Answers\" cell)\n",
    "\n",
    "**2. Get the optimal semivariogram model** based on your parameters defined in step 1:\n",
    "\n",
    "   - **Use the `get_semivariogram_model()`** provided for you in the [Helper functions](#Helper-functions) (read the documentation/docstring of the function for further details how to use it).\n",
    "   - Use the `train` dataset again as the input gdf\n",
    "   - Use the `\"F3\"` or `\"log_F3\"` attribute as value column, depending on whether you transformed the data or not\n",
    "   - Pass the `search_radius`, `max_range`, `weighting`, `number_of_ranges` and `nugget` variables for the function (check the function docstring to know how the parameters are called)\n",
    "   - Save the resulting model that is returned by the `get_semivariogram_model()` into a variable `model`\n",
    "   \n",
    "**This model is used for determining the weights for the interpolation**, and in practice used for doing the interpolation (see next step).    \n",
    "As a result of this, you should end up having something like following (it can look different due to different parameter values, and it's okay):\n",
    "![Semivariogram model](img/semivariogram_model.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f27366-2af7-4428-bdc8-9138fffa98ed",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96bf7e72cbf3f8e139ae515b1e4b2acc",
     "grade": true,
     "grade_id": "cell-863e5d8441c92912",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Search radius (in meters in this case because our points are in metric system)\n",
    "search_radius = 0\n",
    "\n",
    "# The maximum distance between points\n",
    "max_range = 0\n",
    "\n",
    "# Weighting\n",
    "# Possible values: \"equal\", \"closest\", \"furthest\" \n",
    "weighting = \"TEST_DIFFERENT_WEIGHTING\"\n",
    "\n",
    "# Algorithm divides the study extent into n number of ranges and tests \n",
    "# the theoretical model output against the experimental variogram for each range\n",
    "# Note: This can be as it is.\n",
    "number_of_ranges = 20\n",
    "\n",
    "# Nugget - I.e. how much do we allow \"measurement error\" in the original observations\n",
    "# Test different values: 0, 0.02, 0.05 \n",
    "nugget = 0.05\n",
    "\n",
    "# REPLACE THE ERROR BELOW WITH YOUR OWN CODE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294d009-ec7d-4d8f-a517-44389cce41be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Questions\n",
    "\n",
    "*Sufficient description gives you 1 point allocated to problem 3.2*.\n",
    "\n",
    "- **Question 3.2**: Describe why/how did you end up selecting the given search radius value?\n",
    "- **Question 3.3**: Which of the `weighting` works the best? Describe why you ended up using specific `weighting` parameter in your analysis?\n",
    "- **Question 3.4**: Which of the `nugget` values produces the \"best\" result? Why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19eeb87-7059-4913-be70-e742d563fd37",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Answers\n",
    "\n",
    "Answer to the questions by adding text after the `Answer` bullet point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e1920-5110-4075-8466-8c3283211d7b",
   "metadata": {},
   "source": [
    "(hint: double-click this cell to activate editing)\n",
    "\n",
    "- Answer for Q3.2: \n",
    "- Answer for Q3.3: \n",
    "- Answer for Q3.4: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b878cd-5394-401c-84eb-7ef94350f6e2",
   "metadata": {},
   "source": [
    "## Task 3.3 - Interpolate with Ordinary Kriging (2 points)\n",
    "\n",
    "After you have defined and created a semivariogram model in the previous step, we can do the interpolation.\n",
    "\n",
    "**1. Use the `interpolate_ordinary_kriging()` -function** provided for you in the [Helper functions](#Helper-functions) (read the documentation/docstring of the function for further details how to use it). \n",
    "\n",
    "  - Use the `train` dataset as the `known_locations_gdf` for the function (i.e. the data with measured observations)\n",
    "  - Use the `test` dataset as the `unknown_locations_gdf` for the function (i.e. the predictions will be done for these points and stored to this GeoDataFrame)\n",
    "  - Use the semivariogram `model` created in the previous step representing the Theoretical Semivariogram used for data interpolation.\n",
    "  - Use the `F3` (or logged version of it) as the `value_col`, which contain information about the values of the measured observations.\n",
    "  - Give an intuitive name for the paramater `target_col` where the predicted values will be stored, such as `\"predicted_krige\"` or `\"predicted_krige_log\"`\n",
    "  - Give an intuitive name for the parameter `error_col` where the standard error of the predictions will be stored, such as `\"std_error\"` or `\"std_error_log\"`\n",
    "  - With `neighbors_range` parameter, you can control the search radius for the known observations which are used to make the predictions.\n",
    "    - Alternatively with `n_neighbors` parameter, you can control how many closest neighbors of the known points are used for making the predicions.\n",
    "  \n",
    "  \n",
    "**2.** In case you have transformed the data, **back-transform the interpolation predictions and standard errors** by calculating the exponential of all predicted and standard error values:\n",
    "\n",
    "  - Store the back-transformed values into `test` dataset with intuitive column names, such as `predicted_krige` and `std_error`\n",
    "  - You can calculate the exponential by using the [`np.exp()` -function](https://numpy.org/doc/stable/reference/generated/numpy.exp.html)\n",
    "  - After this step, your predicted values are on the same scale as the original values in `\"F3\"` column.\n",
    "  \n",
    "**3. What is the RMSE for the Kriging predictions?**\n",
    "\n",
    "  - Calculate the Root Mean Square Error (RMSE) for the prediction** using the `calculate_RMSE()` -function that is provided for you in [Helper functions](#Helper-functions) (read the documentation/docstring of the function for further details how to use it):\n",
    "\n",
    "  - Pass the `test` GeoDataFrame as the gdf\n",
    "  - Use the `\"F3\"` column for the validation\n",
    "  - Use the `\"predicted_krige\"` column as the `prediction_col` (i.e. the column having the values on the same scale as `\"F3\"`)\n",
    "  - Store the result into a new variable called `krige_rmse`\n",
    "  - Print out the RMSE for the interpolation showing how well the predicted values compare against the known values. The lower the RMSE, the better the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944c138-293f-4e3f-9fc2-e637def41faf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "511232287df5b882c646d6139871b899",
     "grade": true,
     "grade_id": "cell-7abb2a440004a023",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# REPLACE THE ERROR BELOW WITH YOUR OWN CODE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d490e-db6a-49cc-b5b1-5e1b36ac9441",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Questions\n",
    "\n",
    "*Sufficient description gives you 0.5 points allocated to problem 3.3*.\n",
    "\n",
    "- **Question 3.5**: Did the Kriging interpolation perform better than IDW? Justify your answer with a short description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ce505-f221-4fcd-b1b2-67ab5b532d42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Answers\n",
    "\n",
    "Answer to the questions by adding text after the `Answer` bullet point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee324a-c0c6-4ac6-a350-1e134b7162e3",
   "metadata": {},
   "source": [
    "(hint: double-click this cell to activate editing)\n",
    "\n",
    "- Answer for Q3.5: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee482fd4-dd36-4704-a230-2e6d3b0adffa",
   "metadata": {},
   "source": [
    "### Task 3.4 - Create a map with Ordinary Kriging predictions (2 points)\n",
    "\n",
    "**1. Use the `interpolate_ordinary_kriging()` function** and:\n",
    "  \n",
    "  - use the hexagon `grid` as the unknown locations to which we will predict values using IDW\n",
    "  - Use the semivariogram `model` created in the previous step representing the Theoretical Semivariogram used for data interpolation.\n",
    "  - Give an intuitive name for the column where the predicted values will be stored, such as `\"predicted_krige\"` or `\"predicted_krige_log\"`\n",
    "  - Give an intuitive name for the column where the standard error of the predictions will be stored, such as `\"std_error\"` or `\"std_error_log\"`\n",
    "  - Store the result from the function into variable `grid` (i.e. we will update the hexagon grid created earlier)\n",
    "  - If needed, back-transform the values again in a similar manner as in Task 3.3.\n",
    "  \n",
    "**2. Create a map with the predicted values using the hexagon grid**, where the color of the cell should be the predicted values from step 1. Use a colormap `\"Reds\"` that you can define with parameter `cmap`. If you want, you can again add the `test` points on top of this hexagon grid which allows you to easily evaluate visually how well the predictions have worked at different locations. As a result, you should get something like the map below.\n",
    "\n",
    "  - **Hint:** Adding multiple layers with `.explore()` is possible as shown in the [last example on this page](https://geopandas.org/en/stable/docs/user_guide/interactive_mapping.html#Interactive-mapping). The examples on the page also shows how you can adjust different visualization parameters, such as color of the points.\n",
    "\n",
    "**3. Create a map with the standard error values using the hexagon grid**, where the color of the cell should be the standard error values. Use a colormap `\"Blues\"` that you can define with parameter `cmap`. As a result, you should get something like the map below.\n",
    "\n",
    "\\*_Further info related to Question 1: Clusters or trends in the standard errors would indicate that it would be good to try to remove the trends and consider anisotropy in the interpolation process. We won't be considering these things in this exercise, but if you are interested to learn more and take into account anisotropy there is another Python package called [PyKrige](https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/index.html) that [can handle anisotropy](https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/generated/pykrige.ok.OrdinaryKriging.html). The library also provides [Universal Kriging](https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/generated/pykrige.uk.UniversalKriging.html) method allowing to use drift terms._ \n",
    "\n",
    "![Ordinary Kriging predictions example](img/ordinary_kriging_predictions.png)\n",
    "\n",
    "___Example 1__ of the resulting map for the Ordinary Kriging predicted values._\n",
    "\n",
    "\n",
    "![Ordinary Kriging std errors example](img/ordinary_kriging_std_errors.png)\n",
    "\n",
    "___Example 2__ of the resulting map for the Ordinary Kriging standard error values._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b63d5-14b0-483d-be90-97ead8635bbb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c0b7c624bf7d3961f5f97ba312f037c",
     "grade": true,
     "grade_id": "cell-e4bdbe9f8c8b0c9b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# REPLACE THE ERROR BELOW WITH YOUR OWN CODE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b85e4-6768-4c88-885b-d90520de4ee0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Questions\n",
    "\n",
    "*Sufficient description gives you 1 point allocated to problem 3.4*.\n",
    "\n",
    "- **Question 3.6**: Do you think the result based on Ordinary Kriging differs significantly from the IDW interpolation? Give a short explanation.\n",
    "- **Question 3.7**: Do you see any evident patterns in the standard error map? Give a short explanation. \n",
    "- **Question 3.8** What is the purpose of standard error map? Why is it useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86841688-7584-4a23-9e61-093ce146a923",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Answers\n",
    "\n",
    "Answer to the questions by adding text after the `Answer` bullet point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7450db35-22f2-406d-ba9d-7609cb2617fc",
   "metadata": {},
   "source": [
    "(hint: double-click this cell to activate editing)\n",
    "\n",
    "- Answer for Q3.6: \n",
    "- Answer for Q3.7: \n",
    "- Answer for Q3.8: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fe782-66f3-4c1e-ab96-40c1715a7e71",
   "metadata": {},
   "source": [
    "## Problem 4 - How long did it take? Optional feedback (1 point)\n",
    "\n",
    "To help developing the exercises, and understanding the time that it took for you to finish the Exercise, please provide an estimate of how many hours you spent for doing this exercise?\n",
    "\n",
    " - I spent approximately this many hours: **X hours**\n",
    " \n",
    "In addition, if you would like to give any feedback about the exercise (optional), please provide it below:\n",
    "\n",
    " - My feedback:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
